# ğŸš¢ Titanic MLOps â€” End-to-End Machine Learning Pipeline

An end-to-end **MLOps implementation** of the classic Titanic survival prediction problem â€” reimagined as a **production-grade machine learning system**.  
This project emphasizes **data orchestration, feature storage, model training, deployment, and observability**, built with **open-source tools** to simulate a real-world MLOps workflow.

---

## ğŸ§  Overview

Instead of focusing solely on model accuracy, this project highlights the **MLOps architecture** required to automate and monitor the entire lifecycle of a machine learning solution â€” from **data ingestion to model deployment**.

The goal is to demonstrate how traditional ML projects evolve into **scalable, monitored, and automated ML systems** using modern DevOps and data engineering tools.

---

## âš™ï¸ Architecture

```mermaid
flowchart TD
    A[CSV Data Source] -->|Astro Airflow| B[PostgreSQL Database]
    B -->|Feature Extraction| C[Redis Feature Store]
    C -->|Training Pipeline| D[Random Forest Model]
    D -->|Pickle Artifact| E[Flask Web App]
    E -->|User Predictions| F[Prometheus Metrics]
    F -->|Visualization| G[Grafana Dashboard]
    G -->|Feedback Loop| A
```

---

## ğŸ§© Key Components

### 1. **Data Layer**
- **ğŸ—‚ Data Ingestion:**  
  - Managed with **Astro Airflow**, orchestrating automated CSV ingestion into **PostgreSQL**.
  - Uses **MinIO** (S3-compatible storage) to simulate AWS S3 buckets â€” enabling cloud-like behavior while **reducing operational costs**.
- **ğŸ” Data Processing:**  
  - Implements feature engineering pipelines (age bins, family size, cabin flags, etc.) using a **modular class structure** in `data_processing.py`.
  - Ensures data consistency, validation, and version tracking.

### 2. **Feature Store**
- Powered by **Redis**, allowing **low-latency access** to processed data during model training and serving.  
- Acts as a bridge between data ingestion and model training pipelines.

### 3. **Model Pipeline**
- Built using **object-oriented design**:
  - `data_ingestion.py`
  - `data_processing.py`
  - `model_trainer.py`
- Each class includes a **custom logger** and **exception handler** for robustness and traceability.  
- **Training:** Random Forest Classifier with **Randomized Search Cross-Validation** for hyperparameter tuning.  
- **Evaluation:** Based on model **accuracy**, with trained models exported as `.pkl` artifacts for deployment.

### 4. **Web Application**
- Developed with **Flask** (backend) and **HTML + Tailwind CSS** (frontend).  
- Designed to provide a smooth and interactive **user interface** for prediction requests.
- The web UI extracts and computes derived fields (e.g., FamilySize, HasCabin, Title, Age_Fare ratio) on the server side.

ğŸ–¼ Example Interface:

![Titanic Web App Screenshot](./assets/Web%20app.png)

---

### 5. **Monitoring & Observability**
- Integrated **Prometheus** for metrics collection and **Grafana** for real-time visualization.
- Tracks:
  - **Prediction counts**
  - **Data drift metrics**
  - **Model usage and system health**

ğŸ–¼ Example Monitoring Dashboard:

![Grafana Monitoring](./assets/Monitoring.png)

---

## ğŸ“¦ Project Structure

```
TITANIC-MLOPS/
â”‚
â”œâ”€â”€ dags/                     # Airflow DAGs for data pipelines
â”œâ”€â”€ pipeline/                 # Modular pipeline scripts
â”‚
â”œâ”€â”€ src/                      # Core application code
â”œâ”€â”€ templates/                # Web UI (HTML templates)
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ static/                   # CSS & assets (Tailwind)
â”‚
â”œâ”€â”€ application.py             # Flask backend app
â”œâ”€â”€ prometheus.yml             # Prometheus configuration
â”œâ”€â”€ Dockerfile                 # Container setup
â”œâ”€â”€ docker-compose.yml         # Orchestrates all services
â”œâ”€â”€ requirements.txt           # Dependencies
â””â”€â”€ README.md
```

---

## ğŸ§° Tech Stack

| Layer | Technology |
|-------|-------------|
| **Data Orchestration** | Astro Airflow |
| **Storage** | PostgreSQL, MinIO (S3 Simulation) |
| **Feature Store** | Redis |
| **Model Training** | scikit-learn, Random Forest, RandomizedSearchCV |
| **Web Backend** | Flask |
| **Frontend** | HTML + Tailwind CSS |
| **Monitoring** | Prometheus + Grafana |
| **Containerization** | Docker & Docker Compose |
| **Logging & Exceptions** | Custom Logger & Exception Classes |

---

## ğŸš€ Run Locally

1. **Clone this repo**
   ```bash
   git clone https://github.com/DanielEXT10/titanic-mlops.git
   cd titanic-mlops
   ```

2. **Start containers**
   ```bash
   docker-compose up --build
   ```

3. **Access services**
   - Flask app â†’ `http://localhost:5000`
   - Airflow UI â†’ `http://localhost:8080`
   - Grafana â†’ `http://localhost:3000`
   - Prometheus â†’ `http://localhost:9090`
   - MinIO Console â†’ `http://localhost:9001`

---

## ğŸ“ˆ Results

- **Model Accuracy:** ~82%  
- **End-to-End Automation:** Achieved via Airflow DAGs and Redis feature store  
- **Cost Optimization:** MinIO used as local S3 emulator  
- **Real-time Monitoring:** Enabled with Prometheus & Grafana



## ğŸ§© Future Improvements

- Integrate **MLflow** for experiment tracking  
- Deploy Flask app with **Gunicorn + Nginx**  
- Add automated data drift detection and retraining  
- CI/CD integration with **GitHub Actions**

---

## âœ¨ Author

**Daniel Alfonso GarcÃ­a PÃ©rez**  
AI & MLOps Engineer | Data Enthusiast | Automation Innovator  
ğŸ“ Guadalajara, Mexico 

---

## ğŸ›  License

This project is licensed under the **MIT License**.

---

> _â€œTurning a simple model into a production-grade machine learning system â€” thatâ€™s where MLOps begins.â€_
